
# coding:utf8
import os
import ipdb
import torch as t
import torchvision as tv
import tqdm
from model import NetG, NetD
from torchnet.meter import AverageValueMeter

import matplotlib.pyplot as plt
import numpy as np
from torch.autograd import Variable
from torch import autograd
import cv2
import torch.optim as optim
from torchvision.utils import save_image

from ctypes import *  # cdll, c_int

lib = cdll.LoadLibrary('./c_c++_loss/libmathBuf.so')


class callsubBuf(object):
    def __init__(self):
        self.obj = lib.subBuf_new()

    def callcursubBuf(self, data, num, outData):
        lib.subBuf_sub(self.obj, data, num, outData)

    def call_call(self, data1, x, y, data3):
        lib.call_call_call(self.obj, data1, x, y, data3)


from pos_pop import pos_pop

cc = pos_pop(0)


# for i in range(len(line_no_gap)):
#     print(len(line_no_gap[i]))
#     print(line_no_gap[i][0])
# print(line_no_gap[0])

# line_no_gap=t.Tensor(line_no_gap)
# # line_no_gap=t.Tensor(line_no_gap)
# print(line_no_gap[:,0])
# input()

# import torch
# L=[[4,5,6],[1,2,3]]
# L.sort(key=lambda x:(x[0]))
# print(L)
# L=torch.Tensor(L)
# print(L[:,0])


# line=(line-MI)/(MA-MI)
# print((line-MI)/(MA-MI))
# print(line)
# input()

# a=[1,2,3]
# print(np.mean(a),np.std(a))
# print(np.sqrt(2/3))
# input()

# print(len(line)-256*100)
# a=np.random.randint(0,(len(line)-1)-256*100)
# print(a)

# res = list(map(lambda x:float(x), line[a:a+256*100]))
#
# res1=t.tensor(res,dtype=t.float32).reshape(256,100,1,1)
# print(res1)
# input()

# print(line[0:-1])


class Config(object):
    data_path = 'picture_presolve/picture4/'  # 数据集存放路径
    num_workers = 4  # 多进程加载数据所用的进程数
    # image_size = 96  # 图片尺寸
    # image_size = 288
    # batch_size = 256
    image_size = 550
    batch_size = 1
    max_epoch = 1
    lr1 = 2e-4  # 生成器的学习率
    lr2 = 2e-4  # 判别器的学习率
    beta1 = 0.5  # Adam优化器的beta1参数
    gpu = True  # 是否使用GPU
    nz = 100  # 噪声维度
    ngf = 64  # 生成器feature map数
    ndf = 64  # 判别器feature map数

    save_path = 'imgs/'  # 生成图片保存路径

    vis = True  # 是否使用visdom可视化
    env = 'GAN'  # visdom的env
    plot_every = 20  # 每间隔20 batch，visdom画图一次

    debug_file = '/tmp/debuggan'  # 存在该文件则进入debug模式
    d_every = 1  # 每1个batch训练一次判别器
    g_every = 5  # 每5个batch训练一次生成器
    save_every = 1  # 没10个epoch保存一次模型
    # netd_path = 'checkpoints/netd_199.pth'  # 'checkpoints/netd_.pth' #预训练模型
    # netg_path = 'checkpoints/netg_199.pth'  # 'checkpoints/netg_211.pth'

    # 只测试不训练
    gen_img = 'result.png'
    # 从512张生成的图片中保存最好的64张
    # gen_num = 64
    gen_num = 1
    gen_search_num = 512
    gen_mean = 0  # 噪声的均值
    gen_std = 1  # 噪声的方差


opt = Config()


# wgan-gp
def compute_gradient_penalty(D, real_samples, fake_samples):
    """Calculates the gradient penalty loss for WGAN GP"""
    # Random weight term for interpolation between real and fake samples
    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))
    # Get random interpolation between real and fake samples
    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)
    d_interpolates = D(interpolates)
    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)
    # Get gradient w.r.t. interpolates
    gradients = autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=fake,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
    )[0]
    gradients = gradients.view(gradients.size(0), -1)
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gradient_penalty


# Loss weight for gradient penalty
lambda_gp = 10

cuda = True if t.cuda.is_available() else False
Tensor = t.cuda.FloatTensor if cuda else t.FloatTensor


def train(**kwargs):
    for k_, v_ in kwargs.items():
        setattr(opt, k_, v_)

    device = t.device('cuda') if opt.gpu else t.device('cpu')
    if opt.vis:
        from visualize import Visualizer
        vis = Visualizer(opt.env)

    # 数据
    transforms = tv.transforms.Compose([
        tv.transforms.Resize(opt.image_size),
        # tv.transforms.CenterCrop(opt.image_size),
        tv.transforms.ToTensor(),
        tv.transforms.Normalize([1.    ,     0.99770098 ,0.99770099], [2.57899053e-06 ,4.65927561e-02 ,4.65927735e-02])
    ])
    dataset = tv.datasets.ImageFolder(opt.data_path, transform=transforms)
    dataloader = t.utils.data.DataLoader(dataset,
                                         batch_size=opt.batch_size,
                                         shuffle=True,
                                         num_workers=opt.num_workers,
                                         drop_last=True
                                         )

    # 网络
    netg, netd = NetG(opt), NetD(opt)
    map_location = lambda storage, loc: storage
    # if opt.netd_path:
    #     netd.load_state_dict(t.load(opt.netd_path, map_location=map_location))
    # if opt.netg_path:
    #     netg.load_state_dict(t.load(opt.netg_path, map_location=map_location))

    # netd.load_state_dict(t.load("./checkpoints/netd_10999_5_2.pth", map_location=map_location))
    # netg.load_state_dict(t.load("./checkpoints/netg_10999_5_2.pth", map_location=map_location))

    netd.to(device)
    netg.to(device)

    # 定义优化器和损失
    optimizer_g = t.optim.Adam(netg.parameters(), opt.lr1, betas=(opt.beta1, 0.999))
    optimizer_d = t.optim.Adam(netd.parameters(), opt.lr2, betas=(opt.beta1, 0.999))
    criterion = t.nn.BCELoss().to(device)

    lr_scheduler_g = optim.lr_scheduler.StepLR(optimizer_g, step_size=1, gamma=0.95)

    lr_scheduler_d = optim.lr_scheduler.StepLR(optimizer_d, step_size=1, gamma=0.95)

    # 真图片label为1，假图片label为0
    # noises为生成网络的输入
    true_labels = t.ones(opt.batch_size).to(device)
    fake_labels = t.zeros(opt.batch_size).to(device)
    fix_noises = t.randn(opt.batch_size, opt.nz, 1, 1).to(device)
    noises = t.randn(opt.batch_size, opt.nz, 1, 1).to(device)

    errord_meter = AverageValueMeter()
    errorg_meter = AverageValueMeter()

    iii = 0

    pos_x = []
    pos_y = []
    loss1 = []
    loss2 = []
    # epochs = range(opt.max_epoch)
    m = 0
    n = 0
    for epoch in range(200):
        for ii, (img, _) in enumerate(dataloader):
            print("epoch=", epoch, "\t", "ii=", ii)
            real_img = Variable(img.type(Tensor))

            if ii % opt.d_every == 0:
                # 训练判别器
                optimizer_d.zero_grad()
                ## 尽可能的把真图片判别为正确
                # Real images
                real_validity = netd(real_img)
                res1 = t.Tensor(cc[m])
                m += 1
                if (m == len(cc)):
                    m = 0
                # res1 = t.Tensor(line_no_gap[m][n:100 + n])
                # m += 1
                # if (m == len(line_no_gap)):
                #     n += 1
                #     m = 0
                # mm1 = m
                # while (n >= len(line_no_gap[m])):
                #     m += 1
                #     if mm1 == m:
                #         m = 0
                #         n = 0
                #         break
                #     if (m == len(line_no_gap)):
                #         n += 1
                #         m = 0
                res11 = res1.reshape(opt.batch_size, opt.nz, 1, 1)
                # ## 尽可能把假图片判别为错误
                # a1 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)
                # flag1 = 0
                # while flag1 == 0:
                #     for i in range(a1, a1 + opt.batch_size * opt.nz):
                #         flag1 = 1
                #         if abs(line[i] - line[i + 1]) > 0.5:
                #             a1 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)
                #             flag1 = 0
                #             break
                # # if epoch == 0 and ii == 0:
                # if ii == 0:
                #     # if epoch==0 and ii==0:
                #     res1=t.zeros_like(line[a1:a1 + opt.batch_size * opt.nz])
                #     # res1 = line[a1:a1 + opt.batch_size * opt.nz]
                #     for r1,rr1 in enumerate(line[a1:a1 + opt.batch_size * opt.nz]):
                #         res1[r1]=rr1
                # else:
                #     res1 = t.zeros_like(line[a1:a1 + opt.batch_size * opt.nz])
                #     # res1 = line[a1:a1 + opt.batch_size * opt.nz]
                #     for r1, rr1 in enumerate(line[a1:a1 + opt.batch_size * opt.nz]):
                #         res1[r1] = rr1
                #     for q,p in enumerate(line_copy):
                #         if q%2 and ii%2:
                #             res1[q]=p
                #         elif(q%2==0 and ii%2==0):
                #             res1[q]=p
                #
                #     # f1=0
                #     # for k in range(len(line_copy)+1,len(res1)):
                #     #     dec=res1[k]-res1[k-1]
                #     #     if f1==0:
                #     #         res1[k-1]=res1[k-2]
                #     #         f1=1
                #     #     res1[k]=res1[k-1]+dec
                # # print(res1)
                # # res_res1,_=t.sort(res1)
                #
                # res11 = res1.reshape(opt.batch_size, opt.nz, 1, 1)
                #
                # # 特征金字塔
                # line_copy = t.zeros_like(line[a1:a1 + opt.batch_size * opt.nz])
                # # line_copy = line[a1 + epoch+2:a1 + opt.batch_size * opt.nz]
                # for r1, rr1 in enumerate(line[a1:a1 + opt.batch_size * opt.nz]):
                #     line_copy[r1] = rr1
                #
                # # print("line_copy:",line_copy)
                # # print(res1)
                # # print("D",len(line_copy))

                noises.data.copy_(res11)
                fake_img = netg(noises).detach()  # 根据噪声生成假图
                # Fake images
                fake_validity = netd(fake_img)
                ## wgan_gp
                # Gradient penalty
                gradient_penalty = compute_gradient_penalty(netd, real_img, fake_img)
                # Adversarial loss
                d_loss = -t.mean(real_validity) + t.mean(fake_validity) + lambda_gp * gradient_penalty

                d_loss.backward()
                optimizer_d.step()

                t.cuda.empty_cache()

                # print("D  output=", output)

                # loss1.append(d_loss)
                # print(sum(loss1)/len(loss1))
                print("d_loss:   ", d_loss)
                print()

            if ii % opt.g_every == 0:
                # 训练生成器
                optimizer_g.zero_grad()
                res2 = t.Tensor(cc[m])
                m += 1
                if (m == len(cc)):
                    m = 0
                # res2 = t.Tensor(line_no_gap[m][n:100 + n])
                # m += 1
                # if (m == len(line_no_gap)):
                #     n += 1
                #     m = 0
                # mm2 = m
                # while (n >= len(line_no_gap[m])):
                #     m += 1
                #     if mm2 == m:
                #         m = 0
                #         n = 0
                #         break
                #     if (m == len(line_no_gap)):
                #         n += 1
                #         m = 0

                res22 = res2.reshape(opt.batch_size, opt.nz, 1, 1)

                # a2 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)
                #
                # flag2 = 0
                # while flag2 == 0:
                #     for i in range(a2, a2 + opt.batch_size * opt.nz):
                #         flag2 = 1
                #         if abs(line[i] - line[i + 1]) > 0.5:
                #             a2 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)
                #             flag2 = 0
                #             break
                # res2=t.zeros_like(line[a2:a2 + opt.batch_size * opt.nz])
                # for r2,rr2 in enumerate(line[a2:a2 + opt.batch_size * opt.nz]):
                #     res2[r2]=rr2
                # # res2 = line[a2:a2 + opt.batch_size * opt.nz]
                # # print(res2)
                # for q, p in enumerate(line_copy):
                #     if q % 2 and ii % 2:
                #         res2[q] = p
                #     elif (q % 2 == 0 and ii % 2 == 0):
                #         res2[q] = p
                # # f2 = 0
                # # for k in range(len(line_copy) + 1, len(res2)):
                # #     dec = res2[k] - res2[k - 1]
                # #     if f2 == 0:
                # #         res2[k - 1] = res2[k - 2]
                # #         f2 = 1
                # #     res2[k] = res2[k - 1] + dec
                #
                #
                # # 特征金字塔
                # line_copy = t.zeros_like(line[a2:a2 + opt.batch_size * opt.nz])
                # # line_copy = line[a2 + epoch + 2:a2 + opt.batch_size * opt.nz]
                # for r2,rr2 in enumerate(line[a2:a2 + opt.batch_size * opt.nz]):
                #     line_copy[r2]=rr2
                #
                # # print("G",len(line_copy))
                # # print("line_copy:", line_copy)
                # # print(res2)
                # # input()
                #
                # # print(res2)
                #
                # res_res2, _ = t.sort(res2)
                #
                # res22=res_res2.reshape(opt.batch_size, opt.nz, 1, 1)

                noises.data.copy_(res22)

                fake_img = netg(noises)

                fake_validity = netd(fake_img)
                g_loss = -t.mean(fake_validity)

                g_loss.backward()
                optimizer_g.step()

                # loss2.append(g_loss)
                # print("***G***   ",sum(loss2) / len(loss2))
                print("g_loss:   ", g_loss)
                print()
                if ii % 5 == 0:
                    save_image(fake_img.data[:25], "image1/%d.%d.png" % (epoch, ii), nrow=5, normalize=True)

            if (ii + 1) % (1000 * opt.save_every) == 0:
                # 保存模型、图片
                # tv.utils.save_image(fix_fake_imgs.data[:64], '%s/%s.png' % (opt.save_path, epoch), normalize=True,
                #                     range=(-1, 1))
                # iii+=1
                t.save(netd.state_dict(), 'checkpoints/netd_%s_%s_%s.pth' % (ii, iii, epoch))
                t.save(netg.state_dict(), 'checkpoints/netg_%s_%s_%s.pth' % (ii, iii, epoch))
                errord_meter.reset()
                errorg_meter.reset()
            if (ii) % (len(dataloader) - 1) == 0:
                # 保存模型、图片
                # tv.utils.save_image(fix_fake_imgs.data[:64], '%s/%s.png' % (opt.save_path, epoch), normalize=True,
                #                     range=(-1, 1))
                # iii+=1
                t.save(netd.state_dict(), 'checkpoints/netd_%s_%s.pth' % (ii, epoch))
                t.save(netg.state_dict(), 'checkpoints/netg_%s_%s.pth' % (ii, epoch))
                errord_meter.reset()
                errorg_meter.reset()

                # if opt.vis and ii % opt.plot_every == opt.plot_every - 1:
                #     ## 可视化
                #     if os.path.exists(opt.debug_file):
                #         ipdb.set_trace()
                #
                #     a3 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)
                #
                #     flag3 = 0
                #     while flag3 == 0:
                #         for i in range(a3, a3 + opt.batch_size * opt.nz):
                #             flag3 = 1
                #             if abs(line[i] - line[i + 1]) > 1:
                #                 a3 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)
                #                 flag3 = 0
                #                 break
                #
                #
                #     res3 = line[a3:a3 + opt.batch_size * opt.nz].reshape(opt.batch_size, opt.nz, 1, 1)
                #
                #     fix_noises.data.copy_(res3)
                #
                #     fix_fake_imgs = netg(fix_noises)
                #
                #     # plt.imshow(t.tensor(fix_fake_imgs[0]).cpu().permute(1,2,0))
                #     # plt.show()
                #
                #     # grad=fix_fake_imgs[0].mul(255).add_(0.5).clamp_(0, 255).cpu().permute(1,2,0).detach().numpy()
                #     grad = fix_fake_imgs[0].clamp_(0,1).cpu().permute(1, 2, 0).detach().numpy()
                #
                #     plt.imsave('./imgs/' + str(iii) + '.png', grad)
                #
                #     # grad = fix_fake_imgs[0].mul(255).add_(0.5).clamp_(0, 255).cpu().permute(1, 2, 0).detach().numpy()
                #     from PIL import Image
                #     # plt.pause(0.01)
                #     # imgg=t.tensor(fix_fake_imgs[0]).cpu().permute(1, 2, 0)
                #     tv.utils.save_image(fix_fake_imgs.data[0], "./imgs/{}.jpg" .format (iii), normalize=True,
                #                         range=(-1, 1))
                #     # plt.imsave('./imgs/' + str(iii) + '.jpg', imgg[:])
                # #
                # #
                # #
                # #     # print(imgg[0].shape)
                # #     # input()
                # #     # # print(imgg)
                # #     # plt.imsave('./imgs/' + str(iii) + '.jpg', imgg)
                # #     # iii += 1
                # #     # imgg=t.tensor(fix_fake_imgs[0].cpu(), dtype=t.float32).permute(1, 2, 0)[:, :, 1]
                # #     # imgg = t.tensor(fix_fake_imgs[0]).cpu().permute(1,2,0)
                # #     # plt.imsave('./imgs/' + str(iii) + '.jpg', imgg)
                iii += 1

            #
            #     vis.images(fix_fake_imgs.detach().cpu().numpy()[:64] * 0.5 + 0.5, win='fixfake')
            #     vis.images(real_img.data.cpu().numpy()[:64] * 0.5 + 0.5, win='real')
            #     vis.plot('errord', errord_meter.value()[0])
            #     vis.plot('errorg', errorg_meter.value()[0])

        # del line_copy, res1, res11, res2, res22
        lr_scheduler_g.step()
        lr_scheduler_d.step()

        # if (epoch + 1) % opt.save_every == 0:
        #     # 保存模型、图片
        #     # tv.utils.save_image(fix_fake_imgs.data[:64], '%s/%s.png' % (opt.save_path, epoch), normalize=True,
        #     #                     range=(-1, 1))
        #     t.save(netd.state_dict(), 'checkpoints/netd_%s.pth' % epoch)
        #     t.save(netg.state_dict(), 'checkpoints/netg_%s.pth' % epoch)
        #     errord_meter.reset()
        #     errorg_meter.reset()


# @t.no_grad()
# def generate(**kwargs):
#     """
#     随机生成动漫头像，并根据netd的分数选择较好的
#     """
#     for k_, v_ in kwargs.items():
#         setattr(opt, k_, v_)
#
#     device = t.device('cuda') if opt.gpu else t.device('cpu')
#
#     netg, netd = NetG(opt).eval(), NetD(opt).eval()
#
#     # a4 = np.random.randint(0, (len(line) - 1) - opt.batch_size * opt.nz)
#     # res4 = list(map(lambda x: float(x), line[a4:a4 + opt.batch_size * opt.nz]))
#     # res44 = t.tensor(res4, dtype=t.float32).reshape(opt.batch_size, opt.nz, 1, 1)
#     # fix_noises.data.copy_(res44)
#
#     noises = t.randn(opt.gen_search_num, opt.nz, 1, 1).normal_(opt.gen_mean, opt.gen_std)
#     noises = noises.to(device)
#
#     map_location = lambda storage, loc: storage
#     # netd.load_state_dict(t.load(opt.netd_path, map_location=map_location))
#     # netg.load_state_dict(t.load(opt.netg_path, map_location=map_location))
#
#     # netd.load_state_dict(t.load("./checkpoints/netd_19999_2_0.pth", map_location=map_location))
#     # netg.load_state_dict(t.load("./checkpoints/netg_19999_2_0.pth", map_location=map_location))
#     # netd.to(device)
#     # netg.to(device)
#
#     # 生成图片，并计算图片在判别器的分数
#     fake_img = netg(noises)
#     # # print(fake_img.shape)
#     # plt.imshow(fake_img[0])
#     # plt.show()
#     scores = netd(fake_img).detach()
#
#     挑选最好的某几张
#     indexs = scores.topk(opt.gen_num)[1]
#     result = []
#     for ii in indexs:
#         print("*")
#         x = t.tensor(fake_img.data[ii], dtype=t.float32).permute(1, 2, 0)
#
#         result.append(fake_img.data[ii])
#         plt.imshow(x)
#         plt.show()
#     # 保存图片
#     tv.utils.save_image(t.stack(result), opt.gen_img, normalize=True, range=(-1, 1))


def generate(**kwargs):
    for k_, v_ in kwargs.items():
        setattr(opt, k_, v_)
    map_location = lambda storage, loc: storage
    netg, netd = NetG(opt).eval().cuda(), NetD(opt).eval().cuda()
    device = t.device('cuda') if opt.gpu else t.device('cpu')

    netd.load_state_dict(t.load("./picture_presolve/netd_5999_11_5.pth", map_location=map_location))
    netg.load_state_dict(t.load("./picture_presolve/netg_5999_11_5.pth", map_location=map_location))

    netd.to(device)
    netg.to(device)

    noises = t.randn(opt.batch_size, opt.nz, 1, 1).to(device)

    for i in range(30, 10000):
        m = i
        res2 = t.Tensor(cc[m])
        # if res2[99]>=1.1 or res2[99]<=0:
        #     continue
        # m += 1
        if (m == len(cc)):
            m = 0
        # res2 = t.Tensor(line_no_gap[m][n:100 + n])
        # m += 1
        # if (m == len(line_no_gap)):
        #     n += 1
        #     m = 0
        # mm2 = m
        # while (n >= len(line_no_gap[m])):
        #     m += 1
        #     if mm2 == m:
        #         m = 0
        #         n = 0
        #         break
        #     if (m == len(line_no_gap)):
        #         n += 1
        #         m = 0
        #  (x+a) * b
        import random
        res2_left=[]
        # res2_left=-t.rand([25])*1.2  # 1.5-1.0   0.65+
        # bbb=random.randint(0,5)
        flag=100
        while flag!=0:
            # c=random.randint(1,20)
            # d=random.randint(1,random.randint(1,20))
            bbb = random.randint(0, random.randint(1,random.randint(1,20)))
            if flag-bbb>=0:
                flag-=bbb
                ttt=-random.uniform(0,1)
                for i in range(bbb):
                    ttt=ttt-random.uniform(0.01, 0.05)
                    res2_left.append(ttt)
        print(res2_left)
        res2_left=t.tensor(res2_left,dtype=t.float32)
        print(res2_left)
        # print(bbb)
        # input()
        # dd=random.randint(1,30)
        # for i in range(1,dd):
        #     temp=random.uniform(4,6)
        #     res2_left[random.randint(0,29)]=temp
        # for i in range(10):
        #     temp=random.uniform(3,4)
        #     res2_left[random.randint(70,99)]=temp

        dd = random.randint(1, 30)
        for i in range(1, dd):
            temp = random.uniform(-7, -5)
            res2_left[random.randint(0, 29)] = temp
        for i in range(10):
            temp = random.uniform(2, 4)
            res2_left[random.randint(70, 99)] = temp


        # res2_left=res2_left.repeat(4,1).view(-1)
        res2=t.rand([0])
        res2=res2*0.1
        res2=t.cat([res2,res2],0)
        res2_left,_=t.sort(res2_left,0,descending=False)
        res2,_=t.sort(res2,0)
        print("xxx",res2_left)
        print(res2)
        res2_right=t.cat([res2_left,res2],0)
        # res2_right=-t.rand([100])
        # res2_right,_=t.sort(res2_right,0)
        #
        print(res2_right)
        res22 = res2_right.reshape(opt.batch_size, opt.nz, 1, 1)
        print(t.mean(res2_left), t.std(res2_left), t.sum(res2_left))
        # print(t.std(res2[:50]),t.std(res2[50:100]))
        # if t.mean(res2[:50])>-0.2 or t.mean(res2[50:100])<-0.7:
        #     continue



        print(res2.view(-1))
        print("mean=", t.mean(res2))
        noises.data.copy_(res22)

        fake_img = netg(noises)

        plt.imshow(fake_img[0].mul(255).add_(0.5).clamp_(0, 255).cpu().permute(1, 2, 0).detach().numpy())
        # plt.imshow(t.tensor(fake_img[0]).cpu().permute(1,2,0))
        plt.show()

    save_image(fake_img.data[:25], "image1/%d.%d.png" % (3, 2), nrow=5, normalize=True)


if __name__ == '__main__':
    # import fire
    #
    # fire.Fire()

    train(gpu=True, vis=True)
    # generate(gpu=True, vis=True)
#

#
# # -*- coding: utf-8 -*-**
# import numpy as np
# import cv2
# import random
# import os
#
# # calculate means and std  注意换行\n符号**
# # train.txt中每一行是图像的位置信息**
# # path = 'C:/Users/lenovo/PycharmProjects/my/image_list.txt'
# path='./picture_presolve/picture4/picture'
# lines = os.listdir(path)
# # print(filelist1)
# # print(len(filelist1))
# means = [0, 0, 0]
# stdevs = [0, 0, 0]
#
# index = 1
# num_imgs = 0
#
# for line in lines:
#     print(line)
#     print('{}/{}'.format(index, len(lines)))
#     index += 1
#     a = os.path.join(path,line)
#     # print(a[:-1])
#     num_imgs += 1
#     img = cv2.imread(a[:])
#     # print(img)
#     # print(img, 22)
#     img = np.asarray(img)
#
#     img = img.astype(np.float32) / 255.
#     for i in range(3):
#         means[i] += img[:, :, i].mean()
#         stdevs[i] += img[:, :, i].std()
#
# print(num_imgs)
# means.reverse()
# stdevs.reverse()
#
# means = np.asarray(means) / num_imgs
# stdevs = np.asarray(stdevs) / num_imgs
#
# print("normMean = {}".format(means))
# print("normStd = {}".format(stdevs))
# print('transforms.Normalize(normMean = {}, normStd = {})'.format(means, stdevs))
