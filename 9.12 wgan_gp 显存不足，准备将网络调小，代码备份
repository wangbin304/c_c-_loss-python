
loss_wgan_gp_scartch_13.py


# coding:utf8
import os
import ipdb
import torch as t
import torchvision as tv
import tqdm
from model import NetG, NetD
from torchnet.meter import AverageValueMeter

import matplotlib.pyplot as plt
import numpy as np
from torch.autograd import Variable
from torch import autograd
import cv2

from ctypes import *  # cdll, c_int

lib = cdll.LoadLibrary('./c_c++_loss/libmathBuf.so')


class callsubBuf(object):
    def __init__(self):
        self.obj = lib.subBuf_new()

    def callcursubBuf(self, data, num, outData):
        lib.subBuf_sub(self.obj, data, num, outData)

    def call_call(self, data1, x, y, data3):
        lib.call_call_call(self.obj, data1, x, y, data3)


list_file = "./picture_presolve/pos_y.txt"
with open(list_file) as f:
    lines = f.readlines()
    # print(lines)
    line = lines[0].strip().split(".")
line = list(map(lambda x: float(x), line[:-1]))
# MI=np.min(line)
# MA=np.max(line)
mean = np.mean(line)
std = np.std(line)
line = t.Tensor(line)
line = (line - mean) / std


# line=(line-MI)/(MA-MI)
# print((line-MI)/(MA-MI))
# print(line)
# input()

# a=[1,2,3]
# print(np.mean(a),np.std(a))
# print(np.sqrt(2/3))
# input()

# print(len(line)-256*100)
# a=np.random.randint(0,(len(line)-1)-256*100)
# print(a)

# res = list(map(lambda x:float(x), line[a:a+256*100]))
#
# res1=t.tensor(res,dtype=t.float32).reshape(256,100,1,1)
# print(res1)
# input()

# print(line[0:-1])


class Config(object):
    data_path = 'picture_presolve/picture4/'  # 数据集存放路径
    num_workers = 4  # 多进程加载数据所用的进程数
    # image_size = 96  # 图片尺寸
    # image_size = 288
    # batch_size = 256
    image_size = 550
    batch_size = 1
    max_epoch = 1
    lr1 = 2e-4  # 生成器的学习率
    lr2 = 2e-4  # 判别器的学习率
    beta1 = 0.5  # Adam优化器的beta1参数
    gpu = True  # 是否使用GPU
    nz = 100  # 噪声维度
    ngf = 64  # 生成器feature map数
    ndf = 64  # 判别器feature map数

    save_path = 'imgs/'  # 生成图片保存路径

    vis = True  # 是否使用visdom可视化
    env = 'GAN'  # visdom的env
    plot_every = 20  # 每间隔20 batch，visdom画图一次

    debug_file = '/tmp/debuggan'  # 存在该文件则进入debug模式
    d_every = 1  # 每1个batch训练一次判别器
    g_every = 5  # 每5个batch训练一次生成器
    save_every = 10  # 没10个epoch保存一次模型
    # netd_path = 'checkpoints/netd_199.pth'  # 'checkpoints/netd_.pth' #预训练模型
    # netg_path = 'checkpoints/netg_199.pth'  # 'checkpoints/netg_211.pth'

    # 只测试不训练
    gen_img = 'result.png'
    # 从512张生成的图片中保存最好的64张
    # gen_num = 64
    gen_num=1
    gen_search_num = 512
    gen_mean = 0  # 噪声的均值
    gen_std = 1  # 噪声的方差


opt = Config()

# wgan-gp
def compute_gradient_penalty(D, real_samples, fake_samples):
    """Calculates the gradient penalty loss for WGAN GP"""
    # Random weight term for interpolation between real and fake samples
    alpha = t.Tensor(np.random.random((real_samples.size(0), 1, 1, 1))).cuda()
    # Get random interpolation between real and fake samples
    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)
    d_interpolates = D(interpolates)
    fake = Variable(t.Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)
    # Get gradient w.r.t. interpolates
    gradients = autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=fake.cuda(),
        create_graph=True,
        # retain_graph=True,
        # only_inputs=True,
    )[0]
    gradients = gradients.view(gradients.size(0), -1)
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gradient_penalty



def train(**kwargs):
    for k_, v_ in kwargs.items():
        setattr(opt, k_, v_)

    device = t.device('cuda') if opt.gpu else t.device('cpu')
    if opt.vis:
        from visualize import Visualizer
        vis = Visualizer(opt.env)

    # 数据
    transforms = tv.transforms.Compose([
        tv.transforms.Resize(opt.image_size),
        # tv.transforms.CenterCrop(opt.image_size),
        tv.transforms.ToTensor(),
        tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
    ])
    dataset = tv.datasets.ImageFolder(opt.data_path, transform=transforms)
    dataloader = t.utils.data.DataLoader(dataset,
                                         batch_size=opt.batch_size,
                                         shuffle=True,
                                         num_workers=opt.num_workers,
                                         drop_last=True
                                         )

    # 网络
    netg, netd = NetG(opt), NetD(opt)
    map_location = lambda storage, loc: storage
    # if opt.netd_path:
    #     netd.load_state_dict(t.load(opt.netd_path, map_location=map_location))
    # if opt.netg_path:
    #     netg.load_state_dict(t.load(opt.netg_path, map_location=map_location))

    # netd.load_state_dict(t.load("./checkpoints/netd_19999_999_0.pth", map_location=map_location))
    # netg.load_state_dict(t.load("./checkpoints/netg_19999_999_0.pth", map_location=map_location))

    netd.to(device)
    netg.to(device)

    # 定义优化器和损失
    optimizer_g = t.optim.Adam(netg.parameters(), opt.lr1, betas=(opt.beta1, 0.999))
    optimizer_d = t.optim.Adam(netd.parameters(), opt.lr2, betas=(opt.beta1, 0.999))
    criterion = t.nn.BCELoss().to(device)

    # 真图片label为1，假图片label为0
    # noises为生成网络的输入
    true_labels = t.ones(opt.batch_size).to(device)
    fake_labels = t.zeros(opt.batch_size).to(device)
    fix_noises = t.randn(opt.batch_size, opt.nz, 1, 1).to(device)
    noises = t.randn(opt.batch_size, opt.nz, 1, 1).to(device)

    errord_meter = AverageValueMeter()
    errorg_meter = AverageValueMeter()

    iii = 0

    pos_x = []
    pos_y = []
    loss1=[]
    loss2=[]

    epochs = range(opt.max_epoch)
    for epoch in iter(epochs):
        for ii, (img, _) in tqdm.tqdm(enumerate(dataloader)):
            print("epoch=",epoch,"\t","ii=",ii)

            real_img = img.to(device)

            if ii % opt.d_every == 0:
                # 训练判别器
                optimizer_d.zero_grad()
                ## 尽可能的把真图片判别为正确
                # Real images
                real_validity = netd(real_img)


                ## 尽可能把假图片判别为错误
                a1 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)
                flag1=0
                while flag1==0:
                    for i in range(a1,a1+opt.batch_size*opt.nz):
                        flag1=1
                        if abs(line[i]-line[i+1])>1:
                            a1 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)
                            flag1=0
                            break

                res1 = line[a1:a1 + opt.batch_size * opt.nz].reshape(opt.batch_size, opt.nz, 1, 1)

                noises.data.copy_(res1)
                fake_img = netg(noises).detach()  # 根据噪声生成假图
                # Fake images
                fake_validity = netd(fake_img)
                ## wgan_gp
                # Gradient penalty
                gradient_penalty = compute_gradient_penalty(netd, real_img, fake_img)
                # Adversarial loss
                d_loss = -t.mean(real_validity) + t.mean(fake_validity) + 10 * gradient_penalty

                d_loss.backward()
                optimizer_d.step()



                # print("D  output=", output)


                loss1.append(d_loss)
                print(sum(loss1)/len(loss1))
                print()

            if ii % opt.g_every == 0:
                # 训练生成器
                optimizer_g.zero_grad()

                a2 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)

                flag2 = 0
                while flag2 == 0:
                    for i in range(a2, a2 + opt.batch_size * opt.nz):
                        flag2 = 1
                        if abs(line[i] - line[i + 1]) > 1:
                            a2 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)
                            flag2 = 0
                            break


                res2 = line[a2:a2 + opt.batch_size * opt.nz].reshape(opt.batch_size, opt.nz, 1, 1)
                noises.data.copy_(res2)

                fake_img = netg(noises)

                fake_validity = netd(fake_img)
                g_loss = -t.mean(fake_validity)

                g_loss.backward()
                optimizer_g.step()

                loss2.append(g_loss)
                print("***G***   ",sum(loss2) / len(loss2))
                print()


            if (ii + 1) % (1000*opt.save_every) == 0:
                # 保存模型、图片
                # tv.utils.save_image(fix_fake_imgs.data[:64], '%s/%s.png' % (opt.save_path, epoch), normalize=True,
                #                     range=(-1, 1))
                # iii+=1
                t.save(netd.state_dict(), 'checkpoints/netd_%s_%s_%s.pth' % (ii,iii,epoch))
                t.save(netg.state_dict(), 'checkpoints/netg_%s_%s_%s.pth' % (ii,iii,epoch))
                errord_meter.reset()
                errorg_meter.reset()

            if opt.vis and ii % opt.plot_every == opt.plot_every - 1:
                ## 可视化
                if os.path.exists(opt.debug_file):
                    ipdb.set_trace()

                a3 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)

                flag3 = 0
                while flag3 == 0:
                    for i in range(a3, a3 + opt.batch_size * opt.nz):
                        flag3 = 1
                        if abs(line[i] - line[i + 1]) > 1:
                            a3 = np.random.randint(0, len(line) - opt.batch_size * opt.nz)
                            flag3 = 0
                            break


                res3 = line[a3:a3 + opt.batch_size * opt.nz].reshape(opt.batch_size, opt.nz, 1, 1)

                fix_noises.data.copy_(res3)

                fix_fake_imgs = netg(fix_noises)

                # plt.imshow(t.tensor(fix_fake_imgs[0]).cpu().permute(1,2,0))
                # plt.show()

                # grad=fix_fake_imgs[0].mul(255).add_(0.5).clamp_(0, 255).cpu().permute(1,2,0).detach().numpy()
                grad = fix_fake_imgs[0].clamp_(0,1).cpu().permute(1, 2, 0).detach().numpy()

                plt.imsave('./imgs/' + str(iii) + '.png', grad)

                # grad = fix_fake_imgs[0].mul(255).add_(0.5).clamp_(0, 255).cpu().permute(1, 2, 0).detach().numpy()
                from PIL import Image
                # plt.pause(0.01)
                # imgg=t.tensor(fix_fake_imgs[0]).cpu().permute(1, 2, 0)
                tv.utils.save_image(fix_fake_imgs.data[0], "./imgs/{}.jpg" .format (iii), normalize=True,
                                    range=(-1, 1))
                # plt.imsave('./imgs/' + str(iii) + '.jpg', imgg[:])
            #
            #
            #
            #     # print(imgg[0].shape)
            #     # input()
            #     # # print(imgg)
            #     # plt.imsave('./imgs/' + str(iii) + '.jpg', imgg)
            #     # iii += 1
            #     # imgg=t.tensor(fix_fake_imgs[0].cpu(), dtype=t.float32).permute(1, 2, 0)[:, :, 1]
            #     # imgg = t.tensor(fix_fake_imgs[0]).cpu().permute(1,2,0)
            #     # plt.imsave('./imgs/' + str(iii) + '.jpg', imgg)
                iii += 1
            #
            #     vis.images(fix_fake_imgs.detach().cpu().numpy()[:64] * 0.5 + 0.5, win='fixfake')
            #     vis.images(real_img.data.cpu().numpy()[:64] * 0.5 + 0.5, win='real')
            #     vis.plot('errord', errord_meter.value()[0])
            #     vis.plot('errorg', errorg_meter.value()[0])

        if (epoch + 1) % opt.save_every == 0:
            # 保存模型、图片
            # tv.utils.save_image(fix_fake_imgs.data[:64], '%s/%s.png' % (opt.save_path, epoch), normalize=True,
            #                     range=(-1, 1))
            t.save(netd.state_dict(), 'checkpoints/netd_%s.pth' % epoch)
            t.save(netg.state_dict(), 'checkpoints/netg_%s.pth' % epoch)
            errord_meter.reset()
            errorg_meter.reset()


@t.no_grad()
def generate(**kwargs):
    """
    随机生成动漫头像，并根据netd的分数选择较好的
    """
    for k_, v_ in kwargs.items():
        setattr(opt, k_, v_)

    device = t.device('cuda') if opt.gpu else t.device('cpu')

    netg, netd = NetG(opt).eval(), NetD(opt).eval()

    # a4 = np.random.randint(0, (len(line) - 1) - opt.batch_size * opt.nz)
    # res4 = list(map(lambda x: float(x), line[a4:a4 + opt.batch_size * opt.nz]))
    # res44 = t.tensor(res4, dtype=t.float32).reshape(opt.batch_size, opt.nz, 1, 1)
    # fix_noises.data.copy_(res44)

    noises = t.randn(opt.gen_search_num, opt.nz, 1, 1).normal_(opt.gen_mean, opt.gen_std)
    noises = noises.to(device)

    map_location = lambda storage, loc: storage
    # netd.load_state_dict(t.load(opt.netd_path, map_location=map_location))
    # netg.load_state_dict(t.load(opt.netg_path, map_location=map_location))

    # netd.load_state_dict(t.load("./checkpoints/netd_19999_2_0.pth", map_location=map_location))
    # netg.load_state_dict(t.load("./checkpoints/netg_19999_2_0.pth", map_location=map_location))
    # netd.to(device)
    # netg.to(device)

    # 生成图片，并计算图片在判别器的分数
    fake_img = netg(noises)
    # # print(fake_img.shape)
    # plt.imshow(fake_img[0])
    # plt.show()
    scores = netd(fake_img).detach()

    挑选最好的某几张
    indexs = scores.topk(opt.gen_num)[1]
    result = []
    for ii in indexs:
        print("*")
        x = t.tensor(fake_img.data[ii], dtype=t.float32).permute(1, 2, 0)

        result.append(fake_img.data[ii])
        plt.imshow(x)
        plt.show()
    # 保存图片
    tv.utils.save_image(t.stack(result), opt.gen_img, normalize=True, range=(-1, 1))


if __name__ == '__main__':
    # import fire
    #
    # fire.Fire()
    train(gpu=True, vis=True)
    # generate(gpu=True, vis=True)
#








model.py

        # # coding:utf8
        # from torch import nn
        #
        #
        # class NetG(nn.Module):
        #     """
        #     生成器定义
        #     """
        #
        #     def __init__(self, opt):
        #         super(NetG, self).__init__()
        #         ngf = opt.ngf  # 生成器feature map数
        #
        #         self.main = nn.Sequential(
        #             # 输入是一个nz维度的噪声，我们可以认为它是一个1*1*nz的feature map
        #             nn.ConvTranspose2d(opt.nz, ngf * 8, 4, 1, 0, bias=False),
        #             nn.BatchNorm2d(ngf * 8),
        #             nn.ReLU(True),
        #             # 上一步的输出形状：(ngf*8) x 4 x 4
        #
        #             nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
        #             nn.BatchNorm2d(ngf * 4),
        #             nn.ReLU(True),
        #             # 上一步的输出形状： (ngf*4) x 8 x 8
        #
        #             nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
        #             nn.BatchNorm2d(ngf * 2),
        #             nn.ReLU(True),
        #             # 上一步的输出形状： (ngf*2) x 16 x 16
        #
        #             nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
        #             nn.BatchNorm2d(ngf),
        #             nn.ReLU(True),
        #             # 上一步的输出形状：(ngf) x 32 x 32
        #
        #             nn.ConvTranspose2d(ngf, 3, 5, 3, 1, bias=False),
        #             nn.Tanh()  # 输出范围 -1~1 故而采用Tanh
        #             # 输出形状：3 x 96 x 96
        #         )
        #
        #     def forward(self, input):
        #         return self.main(input)


        # class NetD(nn.Module):
        #     """
        #     判别器定义
        #     """
        #
        #     def __init__(self, opt):
        #         super(NetD, self).__init__()
        #         ndf = opt.ndf
        #         self.main = nn.Sequential(
        #
        #
        #         # 输入 3 x 96 x 96
        #         nn.Conv2d(3, ndf, 5, 3, 1, bias=False),
        #         nn.LeakyReLU(0.2, inplace=True),
        #         # 输出 (ndf) x 32 x 32
        #
        #         nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
        #         nn.BatchNorm2d(ndf * 2),
        #         nn.LeakyReLU(0.2, inplace=True),
        #         # 输出 (ndf*2) x 16 x 16
        #
        #         nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
        #         nn.BatchNorm2d(ndf * 4),
        #         nn.LeakyReLU(0.2, inplace=True),
        #         # 输出 (ndf*4) x 8 x 8
        #
        #         nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
        #         nn.BatchNorm2d(ndf * 8),
        #         nn.LeakyReLU(0.2, inplace=True),
        #         # 输出 (ndf*8) x 4 x 4
        #
        #         nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
        #         nn.Sigmoid()  # 输出一个数(概率)
        #  )
        #
        #
        #     def forward(self, input):
        #
        #
        #         return self.main(input).view(-1)
        #


# coding:utf8
from torch import nn


class NetG(nn.Module):
    """
    生成器定义
    """

    def __init__(self, opt):
        super(NetG, self).__init__()
        ngf = opt.ngf  # 生成器feature map数

        self.main = nn.Sequential(
        # 输入是一个nz维度的噪声，我们可以认为它是一个1*1*nz的feature map
        nn.ConvTranspose2d(opt.nz, ngf * 8, 4, 1, 0, bias=False),
        nn.BatchNorm2d(ngf * 8),
        nn.ReLU(True),
        # 上一步的输出形状：(ngf*8) x 4 x 4

        nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
        nn.BatchNorm2d(ngf * 4),
        nn.ReLU(True),
        # 上一步的输出形状： (ngf*4) x 8 x 8

        nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
        nn.BatchNorm2d(ngf * 2),
        nn.ReLU(True),
        # 上一步 16 x 16
        nn.ConvTranspose2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),
        nn.BatchNorm2d(ngf * 4),
        nn.ReLU(True),
        # 上一步 32 x 32

        nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
        nn.BatchNorm2d(ngf * 2),
        nn.ReLU(True),
        # 上一步 64 x 64

        nn.ConvTranspose2d(ngf * 2, ngf * 4, 4, 2, 1, bias=False),
        nn.BatchNorm2d(ngf * 4),
        nn.ReLU(True),
        # 上一步 128 x 128

        nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
        nn.BatchNorm2d(ngf * 2),
        nn.ReLU(True),
        # 上一步 256 x 256

        #  256
        nn.ConvTranspose2d(ngf * 2, ngf * 4, 3, 2, 0, bias=False),
        nn.BatchNorm2d(ngf * 4),
        nn.ReLU(True),

        # 513

        # nn.ConvTranspose2d(ngf * 4, ngf * 2, 5, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf * 2),
        # nn.ReLU(True),
        # nn.ConvTranspose2d(ngf * 2, ngf * 2, 3, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf * 2),
        # nn.ReLU(True),
        # nn.ConvTranspose2d(ngf * 2, ngf * 4, 5, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf * 4),
        # nn.ReLU(True),
        # nn.ConvTranspose2d(ngf * 4, ngf * 4, 3, 2, 0, bias=False),
        # nn.BatchNorm2d(ngf * 4),
        # nn.ReLU(True),
        # nn.ConvTranspose2d(ngf * 4, ngf * 2, 5, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf * 2),
        # nn.ReLU(True),
        # nn.ConvTranspose2d(ngf * 2, ngf * 2, 3, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf * 2),
        # nn.ReLU(True),
        # nn.ConvTranspose2d(ngf * 2, ngf * 4, 3, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf * 4),
        # nn.ReLU(True),
        # nn.ConvTranspose2d(ngf * 4, 3, 3, 1, 0, bias=False),
        # nn.Tanh()

        # # 515
        # nn.ConvTranspose2d(ngf * 2, ngf * 4, 3, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf * 4),
        # nn.ReLU(True),
        #
        #
        nn.ConvTranspose2d(ngf * 4, ngf * 2, 5, 1, 0, bias=False),
        nn.BatchNorm2d(ngf * 2),
        nn.ReLU(True),
        nn.ConvTranspose2d(ngf * 2, ngf * 4, 5, 1, 0, bias=False),
        nn.BatchNorm2d(ngf * 4),
        nn.ReLU(True),
        nn.ConvTranspose2d(ngf * 4, ngf * 2, 5, 1, 0, bias=False),
        nn.BatchNorm2d(ngf * 2),
        nn.ReLU(True),
        nn.ConvTranspose2d(ngf * 2, ngf * 2, 4, 1, 0, bias=False),
        nn.BatchNorm2d(ngf * 2),
        nn.ReLU(True),
        nn.ConvTranspose2d(ngf * 2, ngf * 2, 5, 1, 0, bias=False),
        nn.BatchNorm2d(ngf * 2),
        nn.ReLU(True),
        nn.ConvTranspose2d(ngf * 2, ngf * 4, 5, 1, 0, bias=False),
        nn.BatchNorm2d(ngf * 4),
        nn.ReLU(True),
        nn.ConvTranspose2d(ngf * 4, ngf * 2, 5, 1, 0, bias=False),
        nn.BatchNorm2d(ngf * 2),
        nn.ReLU(True),
        nn.ConvTranspose2d(ngf * 2, ngf, 4, 1, 0, bias=False),
        nn.BatchNorm2d(ngf),
        nn.ReLU(True),
        nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        nn.BatchNorm2d(ngf),
        nn.ReLU(True),

        nn.ConvTranspose2d(ngf, 3, 5, 1, 0, bias=False),
        nn.Tanh()

        # nn.ConvTranspose2d(ngf * 2, 3, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf * 4),
        # nn.ReLU(True),
        # # 上一步 512 x 512
        #
        # nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),
        # nn.BatchNorm2d(ngf * 2),
        # nn.ReLU(True),  # 1024 x 1024
        # # 上一步的输出形状： (ngf*2) x 16 x 16
        #
        # # nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),
        # # nn.BatchNorm2d(ngf),
        # # nn.ReLU(True),
        # # # 上一步的输出形状：(ngf) x 32 x 32
        #
        # nn.ConvTranspose2d(ngf * 2, 3, 77, 1, 0, bias=False),
        # nn.Tanh()
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1027
        #
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        #
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf*2, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, ngf, 4, 1, 0, bias=False),
        # nn.BatchNorm2d(ngf),
        # nn.ReLU(True),
        # # 1030
        # nn.ConvTranspose2d(ngf, 3, 4, 1, 0, bias=False),
        # nn.Tanh()
        # # # 1030

        # nn.ConvTranspose2d(ngf, 3, 5, 3, 1, bias=False),
        # nn.Tanh()  # 输出范围 -1~1 故而采用Tanh
        # 输出形状：3 x 96 x 96
        )

    def forward(self, input):
        return self.main(input)

#  self
class NetD(nn.Module):
    """
    判别器定义
    """

    def __init__(self, opt):
        super(NetD, self).__init__()
        ndf = opt.ndf

        # self.main = nn.Sequential(

        # self.conv1 = nn.Conv2d(3, 3, 5, 3, 1, bias=False)
        # self.lr1 = nn.LeakyReLU(0.2, inplace=True)

        self.conv2 = nn.Conv2d(3, ndf, 5, 3, 1, bias=False)
        self.lr2 = nn.LeakyReLU(0.2, inplace=True)

        # 输入 3 x 96 x 96
        self.conv3 = nn.Conv2d(ndf, ndf, 5, 3, 1, bias=False)
        self.lr3 = nn.LeakyReLU(0.2, inplace=True)
        # 输出 (ndf) x 32 x 32

        self.conv4 = nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(ndf * 2)
        self.lr4 = nn.LeakyReLU(0.2, inplace=True)
        # 输出 (ndf*2) x 16 x 16

        self.conv5 = nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False)
        self.bn2 = nn.BatchNorm2d(ndf * 4)
        self.lr5 = nn.LeakyReLU(0.2, inplace=True)
        # 输出 (ndf*4) x 8 x 8

        self.conv6 = nn.Conv2d(ndf * 4, ndf * 2, 3, 1, 0, bias=False)
        self.bn3 = nn.BatchNorm2d(ndf * 2)
        self.lr6 = nn.LeakyReLU(0.2, inplace=True)

        self.conv7 = nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 0, bias=False)
        self.bn4 = nn.BatchNorm2d(ndf * 2)
        self.lr7 = nn.LeakyReLU(0.2, inplace=True)

        self.conv8 = nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 0, bias=False)
        self.bn5 = nn.BatchNorm2d(ndf * 2)
        self.lr8 = nn.LeakyReLU(0.2, inplace=True)

        self.conv9 = nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 0, bias=False)
        self.bn6 = nn.BatchNorm2d(ndf * 2)
        self.lr9 = nn.LeakyReLU(0.2, inplace=True)

        self.conv10 = nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 0, bias=False)
        self.bn7 = nn.BatchNorm2d(ndf * 2)
        self.lr10 = nn.LeakyReLU(0.2, inplace=True)

        self.conv11 = nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 0, bias=False)
        self.bn8 = nn.BatchNorm2d(ndf * 2)
        self.lr11 = nn.LeakyReLU(0.2, inplace=True)

        self.conv12 = nn.Conv2d(ndf * 2, 1, 3, 1, 0, bias=False)
        ####### self.sigmoid = nn.Sigmoid()  # 输出一个数(概率)

        # self.bn3 = nn.BatchNorm2d(ndf * 8)
        # self.lr6 = nn.LeakyReLU(0.2, inplace=True)
        # # 输出 (ndf*8) x 4 x 4
        #
        # self.conv7 = nn.Conv2d(ndf * 8, 1, 15, 1, 0, bias=False)
        # self.sigmoid = nn.Sigmoid()  # 输出一个数(概率)

        # self.conv6 = nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)
        # self.bn3 = nn.BatchNorm2d(ndf * 8)
        # self.lr6 = nn.LeakyReLU(0.2, inplace=True)
        # # 输出 (ndf*8) x 4 x 4
        #
        # self.conv7 = nn.Conv2d(ndf * 8, 1, 15, 1, 0, bias=False)
        # self.sigmoid = nn.Sigmoid()  # 输出一个数(概率)

        # )

    def forward(self, input):
        # input = self.conv1(input)
        # input = self.lr1(input)

        input = self.conv2(input)
        input = self.lr2(input)

        input = self.conv3(input)
        input = self.lr3(input)

        input = self.conv4(input)
        input = self.bn1(input)
        input = self.lr4(input)

        input = self.conv5(input)
        input = self.bn2(input)
        input = self.lr5(input)

        input = self.conv6(input)
        input = self.bn3(input)
        input = self.lr6(input)

        input = self.conv7(input)
        input = self.bn4(input)
        input = self.lr7(input)

        input = self.conv8(input)
        input = self.bn5(input)
        input = self.lr8(input)

        input = self.conv9(input)
        input = self.bn6(input)
        input = self.lr9(input)

        input = self.conv10(input)
        input = self.bn7(input)
        input = self.lr10(input)

        input = self.conv11(input)
        input = self.bn8(input)
        input = self.lr11(input)
        input = self.conv12(input)
        # input = self.bn3(input)
        # input = self.lr6(input)
        #
        # # print("self.conv6", input.shape)
        #
        # input = self.conv7(input)
        #######
        # input = self.sigmoid(input)
        #######
        # print("self.conv7", input.shape)

        # return input.view(-1)
        return input.view(input.size(0),-1)

        # return self.main(input).view(-1)








